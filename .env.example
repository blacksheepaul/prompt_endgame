# Server Configuration
# Address to bind the HTTP server (default: :8080)
SERVER_ADDR=:8080

# Server read timeout as a duration string (e.g., "15s", "30s", "1m")
# Default: 15s
SERVER_READ_TIMEOUT=15s

# Server write timeout as a duration string
# Default: 0 (no timeout, recommended for SSE)
SERVER_WRITE_TIMEOUT=0

# LLM Provider Configuration
# Provider type: "mock" (for testing), "openai", "anthropic", etc.
# Default: mock
PROVIDER_TYPE=mock

# Token delay for mock provider as a duration string
# Simulates LLM response time (e.g., "50ms", "100ms", "1s")
# Default: 50ms
PROVIDER_TOKEN_DELAY=50ms

# Scenery Configuration
# Base path for scenery files
# Default: ./sceneries
SCENERY_PATH=./sceneries

# For production use with external LLM providers, you would add:
# OPENAI_API_KEY=your-openai-key-here
# ANTHROPIC_API_KEY=your-anthropic-key-here
